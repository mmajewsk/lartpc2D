#+TITLE: Research
* Notes
** Reinforcement learning
- https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts
  key concepts for reinforcement learning

** Emergence and Autocurricula
Links:
 - https://openai.com/blog/emergent-tool-use/ - Openai Hide and seek
 - https://arxiv.org/pdf/1903.00742.pdf - "Autocurricula and the Emergence
   of Innovation from Social Interaction: A Manifesto for Multi-Agent
   Intelligence Research" general description of autocurricula
 - https://arxiv.org/pdf/1909.07528.pdf - "emergent tool use from multi-agent
autocurricula"  openai Hide and seek paper
*Emergence* - when strategies develop on their own with the RL
*Autocurricula* - the process where strategies are emerging

** A2C - Actor Critic methods
Links:
- https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752
that one cartoon with a fox
- https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf - the paper of actor-critic
- https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f -
  really good article presentic advantage A2C along a solution fort cartpole
*Actor - Critic*
There are two networks, one estimates a value response from the network, the other

** Deep Reinforcement learning
-https://jerrybai1995.github.io/asset/doom/doom_report_final.pdf that doom paper
-https://github.com/flyyufelix/VizDoom-Keras-RL that doom repo
-https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c
-https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288
that cartpole example

** Deep Recurent Q-Networks
- https://arxiv.org/pdf/1507.06527.pdf DRQN

** Normalization of the rewards
- https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63 -
this is about implementing REINFORCE, but there is quote from Karpathy about
normalisation.
#+NAME: karpathy
- http://karpathy.github.io/2016/05/31/rl/ - the source of the quote
- https://arxiv.org/abs/1506.02438 - referenced source of the normalisation

** Stochastic Policy

* To read
What to read:
- [ ] [[karpathy]]'s text on reinforcement
- [ ] "intrinsic motivation" - ?? reading http://www.cs.cornell.edu/~helou/IMRL.pdf on this.
- [ ] "Proximal Policy Optimization Algorithms" https://arxiv.org/pdf/1707.06347.pdf
- [ ] HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION https://arxiv.org/pdf/1506.02438.pdf
- [ ] kinda related maze: "LEARNING TO NAVIGATE
IN COMPLEX ENVIRONMENTS" https://arxiv.org/pdf/1611.03673.pdf,
- [ ] EPISODIC CURIOSITY THROUGH REACHABILITY https://arxiv.org/pdf/1810.02274.pdf
- [ ] Must read on multiple objectives: "Multi-task Deep Reinforcement Learning with PopArt" https://arxiv.org/pdf/1809.04474.pdf

* Ideas
- Cooperative approach
- Sampling distribution of decision ? **

* Paste box
https://stats.stackexchange.com/questions/284265/understanding-median-frequency-balancing
|
V
https://arxiv.org/pdf/1411.4734.pdf
also: Kampffmeyer_Semantic_Segmentation_of_CVPR_2016_paper.pdf
https://stackoverflow.com/questions/42591191/keras-semantic-segmentation-weighted-loss-pixel-map?rq=1
https://github.com/kwotsin/TensorFlow-ENet/blob/master/get_class_weights.py

